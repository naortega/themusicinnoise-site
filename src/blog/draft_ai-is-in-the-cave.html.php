<?php
require 'config.php';
require 'draft_ai-is-in-the-cave.cfg.php';
require 'templates/blog-header.php';
?>
<p>
For your average technological layman most digital technologies seem to pretty
much be magic. Somehow a bunch of ones and zeros can be transformed into a movie
which is streamed over cables and even thin-air until it reaches my tablet and
is transformed to something actually recognizable on the screen. So it should
come as no surprise that when it comes to Large Language Models (LLMs, a.k.a.
AI) the usual amazement and mystification of technology reached new peaks as
folks started speculating that perhaps this is truly the point where computers
can pass the Turing Test, and the more enthusiastic among these would go
further and say that perhaps even the machines could gain
conciousness<sup><a href="#n1" >(1)</a></sup> if they have not already, all
ultimately raising the question of whether we have reached the point of
Singularity.<sup><a href="#n2" >(2)</a></sup> In fact, we could say that the
capabilities of LLMs are so amazing that even some of the CEOs of these
companies are suggesting they either have or may soon truly achieve
consciousness.<sup><a href="#r1" >[1]</a></sup> However, if you were to ask
these people how we can know that LLMs have reached this point the test
provided is generally a very unsophisticated one: if it can ape human behavior,
it must have an intellect at the same level as a human.
</p>

<p>
Now, for anyone who has some familiarity with Pro-Life apologetics, this kind
of reasoning to determine personhood sounds extremely familiar, but is now
being applied in the reverse. It is the attribution of personhood on the basis
of what the thing <em>can do</em> in a specific stage of its development rather
than on the basis of what <em>kind of being</em> the thing is. In the case of
the unborn it is used to claim they do not have personhood because at that
stage of their development they cannot do certain things, while in the case of
LLMs it is used to claim they do have personhood because they can do these
things (at least those things which we associate with the intellect). All this
because our modern materialist culture cannot actually understand the concept
of a <em>kind</em> of being, for all being is merely an assortment of atoms
that just so happen to organize themselves in such a way that a conscious being
is formed. Thus, to form other conscious beings all you have to do is put the
same kinds of atoms together in the same pattern and you can reproduce life!
And not any sort of life, but rational life at that. Furthermore, in the case
of LLMs, it would seem that it is not even necessary for it to be the same
kinds of atoms in the same pattern at all, but instead we can replace neurons
and alike with transistors and other electronic elements, coded to interact
with each other to do the same thing human beings have been doing for tens of
thousands of years, and what pretty much most animals have been able to do as
well: pattern recognition and replication.
</p>

<p>
The fact of the matter is that LLMs may seem, from a purely superficial
standpoint, like a child who is slowly learning to speak. In the past few years
we have seen drastic improvements as many of the tell-tale signs have been
smoothed out of the algorithms. But even as they become indistinguishable from
the product of actual human work, that does not mean that the means of arriving
at that product are the same, and thus the value of the product itself is not
the same. So we must ask ourselves: how <em>do</em> these LLMs work?
</p>

<h2>The Man Who “Learned” Chinese</h2>

<p>
It is not hard to find explanations on the Web that explain in a very technical
manner how these LLMs work, but for most people these explanations are as good
as a neuroscientist explaining how the brain works (which, at least for me,
would be pretty useless). Luckily, it is not necessary to know how all the
gears in an analog watch are interconnected versus the circuits in a digital
watch in order to understand the principle of an analog watch's movement is
kinetic energy and components pushing one another, whereas with the digital
watch it is electrical signals passed through logical circuit components. The
specifics do not really matter for these purposes. Therefore, for LLMs, I would
like to offer an explanation of this principle through analogy which may be
easier for people to understand.
</p>

<p>
Imagine there is a man who is a monolingual English speaker. Furthermore, he
has no knowledge of grammatical concepts which would allow him to think
abstractly about his language, much less any foreign language. Now let us say
you gave this man hundreds, maybe thousands, or maybe even millions of years to
look over Chinese texts. Some of them are books, fiction or non-fiction, some
are articles, some are conversations, others are instruction manuals, etc. All
sorts of texts of practically any kind. After such a long time he begins to
notice some patterns, where normally certain symbols are followed by certain
other symbols. And after all this time you begin to train him: you give him a
text in Chinese and he has to try to return the proper pattern of symbols which
ought to follow the ones you gave him. You, the one who knows Chinese, judge
whether the response makes sense, and if so you give him positive feedback
which tells him he did a good job (and will likely return similar responses for
similar prompts), and if not you give him negative feedback which tells him he
did a bad job (and will be less likely to return similar responses for similar
prompts). After all this time, you finally have trained this man to the point
where if any Chinese person were to speak with him over text prompts he could
respond as if he spoke perfect Chinese and was truly having a conversation or
writing meaningful texts. But is he?
</p>

<p>
If you were to actually ask this man (in English) whether he had any idea what
he was saying, he would obviously reply with a flat “Of course not,” but if you
were to ask him in Chinese I do not think any of us would doubt that he would
simply reply that he does, even though he clearly does not. The man has not
actually learned Chinese, but rather to mimic Chinese. The reason for this is
that he is not capable of doing the very thing that language is meant to do:
convey <em>meaning</em>. Sure, a Chinese speaker may find meaning in the texts
he writes, but that meaning is not his meaning. This language is no longer one
rational agent communicating his ideas to another rational agent, but merely a
single rational agent trying to induce a meaning into a text that wasn't
infused with meaning to begin with.
</p>

<p>
It is from this understanding that some of the various flaws of LLMs begin to
make sense. For example, the reason why they cannot accurately cite sources is
because, firstly, they do not know what a source even is, but secondly, because
it is simply looking back into its data and checking what usually follows
within that context with the parameters of “citing the source,” which is why it
so often simply makes them up. The truth is that its source is <em>all</em> its
data mashed together probabilistically based on the input prompt and the
context of the overall “conversation.”
</p>

<p>
Getting back, however, to a question raised earlier about the level of
consciousness of these LLMs, although in the analogy given above the man surely
has a rational soul and a human intellect, it is also evident how it is not
necessary to make use of these higher faculties in order to do what these
machines can do: it is merely pattern recognition and probabilistic
computation. This is something that even the beasts could do if sufficiently
trained (think of the example of a parrot). The machine has no concept of the
<em>meaning</em> symbolized by these words; in fact, it does not see them as
symbols at all, but tokens with numerical values. And if it cannot comprehend
meaning, then it certainly cannot reason on the basis of meaning. What is more,
this is not simply a question of needing more training or more data, it is a
matter of the process itself; for no matter how much you train the man in the
analogy with more data and better pattern recognition techniques, he never will
have actually learned Chinese until he starts to associate meaning those
symbols and is thus able to reason a response instead of merely guessing what
tokens go next.
</p>

<h2>Here Be Demons</h2>

<h2>Hammers Are for Nails</h2>

<h2>Resources</h2>
<h3>Notes</h3>
<ol class="notes" >
	<li id="n1" >
		Some people confuse the Turing Test to be an indicator that a machine
		<em>has</em> reached conciousness, but this is a misunderstanding. The
		test merely indicates that in a blind-folded scenario a human cannot
		tell whether they are talking to another human or a machine, usually via
		a text prompt.
	</li>
	<li id="n2" >
		Singularity in this context is understood as a point-of-no-return past
		which the advances of technological complexity are beyond our control.
	</li>
</ol>
<h3>References</h3>
<ol class="refs" >
	<li id="r1" ><a href="https://arstechnica.com/ai/2025/03/anthropics-ceo-wonders-if-future-ai-should-have-option-to-quit-unpleasant-tasks/" target="_blank" >Anthropic CEO floats idea of giving AI a “quit job” button, sparking skepticism - Ars Technica</a></li>
</ol>
<?php
require 'templates/blog-footer.php';
?>
